name: python-final


services:
  db_postgresql:
    container_name: postgres_app
    build: ./_CI/db/postgresql
    restart: always
    ports:
      - ${DB_POSTGRES_PORT_IN}:${DB_POSTGRES_PORT_OUT}
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_DB=${DB_POSTGRES_NAME_DB}
      - POSTGRES_USER=${DB_POSTGRES_USER}
      - POSTGRES_PASSWORD=${DB_POSTGRES_PASSWORD}

  db_mysql:
    container_name: mysql_app
    build: ./_CI/db/mysql
    restart: always
    ports:
      - ${DB_MYSQL_PORT_IN}:${DB_MYSQL_PORT_OUT}
    volumes:
      - mysql_data:/var/lib/mysql
    environment:
      - MYSQL_ROOT_PASSWORD=${DB_POSTGRES_PASSWORD}
      - MYSQL_DATABASE=${DB_POSTGRES_NAME_DB}
      - MYSQL_USER=${DB_POSTGRES_USER}
      - MYSQL_PASSWORD=${DB_POSTGRES_PASSWORD}

  data_generator:
    container_name: data_generator_app
    build:
      context: ./
      dockerfile: ./_CI/generator/Dockerfile
    depends_on:
      - db_postgresql

  airflow-init:
    build:
      context: ./
      dockerfile: ./_CI/airflow/init/Dockerfile
    restart: "no"
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////usr/local/airflow/db/airflow.db
    volumes:
      - airflow:/usr/local/airflow/db/:rw

  airflow-webserver:
    image: apache/airflow:2.10.4-python3.12
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////usr/local/airflow/db/airflow.db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/spark_scripts:/opt/airflow/scripts
      - airflow:/usr/local/airflow/db:rw
    ports:
      - "8080:8080"
    depends_on:
      - airflow-scheduler
      - airflow-init
    command: ["webserver"]

  airflow-scheduler:
    build:
      context: ./
      dockerfile: ./_CI/airflow/scheduler/Dockerfile
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////usr/local/airflow/db/airflow.db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/spark_scripts:/opt/airflow/scripts
      - airflow:/usr/local/airflow/db/:rw
    depends_on:
      - airflow-init

  spark:
    image: bitnami/spark:3.5.3
    restart: always
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_WEBUI_PORT=8081
    ports:
      - "8081:8081"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/spark_scripts:/opt/airflow/scripts
      - spark:/opt/spark

  spark-worker:
    image: bitnami/spark:3.5.3
    restart: always
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/spark_scripts:/opt/airflow/scripts
      - spark:/opt/spark
    depends_on:
      - spark

volumes:
  postgres_data:
  mysql_data:
  airflow:
  spark: